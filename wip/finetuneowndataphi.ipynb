{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFCx6jZU3m11"
      },
      "source": [
        "<!-- Banner Image -->\n",
        "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brev-xmas-3.png\" width=\"100%\">\n",
        "\n",
        "<!-- Links -->\n",
        "<center>\n",
        "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> â€¢\n",
        "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> â€¢\n",
        "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> â€¢\n",
        "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
        "</center>\n",
        "\n",
        "# Fine-tuning Microsoft's Phi-2 on your own data ðŸ¤™\n",
        "\n",
        "Welcome!\n",
        "\n",
        "In this notebook and tutorial, we will fine-tune [Microsoft's Phi-2](https://huggingface.co/microsoft/phi-2) relatively small 2.7B model - which has \"showcased a nearly state-of-the-art performance among models with less than 13 billion parameters\" - ***on your own data!***\n",
        "\n",
        "## Watch the accompanying video walk-through (but for Mistral 7B) [here](https://youtu.be/kmkcNVvEz-k?si=Ogt1wRFNqYI6zXfw&t=1)! \n",
        "If you'd like to see a notebook to fine-tune Phi-2 on a Hugging Face dataset instead, click [here](https://github.com/brevdev/notebooks/blob/main/phi2-finetune-own-data.ipynb).\n",
        "\n",
        "I did this for **just one dollar ($1)** on an 1x A10G 24GB from Brev.dev (instructions below).\n",
        "\n",
        "This tutorial will use QLoRA, a fine-tuning method that combines quantization and LoRA. For more information about what those are and how they work, see [this post](https://brev.dev/blog/how-qlora-works).\n",
        "\n",
        "Note that if you ever have trouble importing something from Huggingface, you may need to run `huggingface-cli login` in a shell. To open a shell in Jupyter Lab, click on 'Launcher' (or the '+' if it's not there) next to the notebook tab at the top of the screen. Under \"Other\", click \"Terminal\" and then run the command.\n",
        "\n",
        "### Help us make this tutorial better! Please provide feedback on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9TytWkb3m15"
      },
      "source": [
        "#### Before we begin: A note on OOM errors\n",
        "\n",
        "If you get an error like this: `OutOfMemoryError: CUDA out of memory`, tweak your parameters to make the model less computationally intensive. I will help guide you through that in this guide, and if you have any additional questions you can reach out on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/harperscarroll).\n",
        "\n",
        "To re-try after you tweak your parameters, open a Terminal ('Launcher' or '+' in the nav bar above -> Other -> Terminal) and run the command `nvidia-smi`. Then find the process ID `PID` under `Processes` and run the command `kill [PID]`. You will need to re-start your notebook from the beginning. (There may be a better way to do this... if so please do let me know!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC-9m2yv3m18"
      },
      "source": [
        "## Let's begin!\n",
        "### 0. Preparing data\n",
        "\n",
        "Before you check out a GPU, prepare your dataset for loading and training.\n",
        "\n",
        "To prepare your dataset for loading, all you need are two `.jsonl` files structured something like this:\n",
        "```\n",
        "{\"input\": \"What color is the sky?\", \"output\": \"The sky is blue.\"}\n",
        "{\"input\": \"Where is the best place to get cloud GPUs?\", \"output\": \"Brev.dev\"}\n",
        "```\n",
        "If you choose to model your data as input/output pairs, you'll want to use something like the second `formatting_func` below, which will will combine all your features into one input string.\n",
        "\n",
        "As you can see below, I have `notes.jsonl` for my `train_dataset` and `notes_validation.jsonl` for my `eval_dataset`.\n",
        "\n",
        "I used Exporter, a free local-only app, to export my Apple Notes to `.txt` files, and then I wrote a script to process each note into one `.jsonl` file. Note that for this script, ChatGPT can help out a LOT if you tell it how your data is currently formatted, how you'd like it to be formatted, and ask it to write a script in a certain language you know well (for any debugging) to do so. I also broke up my journal entries so the training sample vector length was smaller (see the discussion on `max_length` and the data visualization below). I broke it into pieces so that contexts were encapsulated entirely, since I did want the model to understand context about my life. My data were ultimately formatted as:\n",
        "\n",
        "```json\n",
        "{\"note\": \"journal-entry-for-model-to-predict\"}\n",
        "{\"note\": \"journal-entry-for-model-to-predict-1\"}\n",
        "{\"note\": \"journal-entry-for-model-to-predict-2\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2CkxsA43m15"
      },
      "source": [
        "### 1. Set Up GPU\n",
        "\n",
        "I used a GPU and dev environment from [brev.dev](https://brev.dev). The whole thing cost me $1 using a 1xA10G 24GB. Click the badge below to get your preconfigured instance:\n",
        "\n",
        "[![click here to deploy](https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg)](https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&diskStorage=256&name=phi2-finetune-own-data&file=https://github.com/brevdev/notebooks/raw/main/phi2-finetune-own-data.ipynb&python=3.10&cuda=12.0.1)\n",
        "\n",
        "A single A10G (as linked) with 24GB GPU Memory was enough for me. You may need more GPUs and/or Memory if your sequence max_length is larger than 512.\n",
        "\n",
        "Once you've checked out your machine and landed in your instance page, select the specs you'd like (I used **Python 3.10 and CUDA 12.0.1**; these should be preconfigured for you if you use the badge above) and click the \"Build\" button to build your verb container. Give this a few minutes.\n",
        "\n",
        "A few minutes after your model has started Running, click the 'Notebook' button on the top right of your screen once it illuminates (you may need to refresh the screen). You will be taken to a Jupyter Lab environment, where you can upload this Notebook.\n",
        "\n",
        "\n",
        "Note: You can connect your cloud credits (AWS or GCP) by clicking \"Org: \" on the top right, and in the panel that slides over, click \"Connect AWS\" or \"Connect GCP\" under \"Connect your cloud\" and follow the instructions linked to attach your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FuXIFTFapAMI",
        "outputId": "c8ced1ad-c7b3-44ba-807b-26d7d13906bc"
      },
      "outputs": [],
      "source": [
        "# # You only need to run this once per machine\n",
        "# !pip install -q -U bitsandbytes\n",
        "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "# !pip install -q -U datasets scipy ipywidgets matplotlib einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05H5MIfjyRgc"
      },
      "source": [
        "#### Accelerator\n",
        "\n",
        "Set up the Accelerator. I'm not sure if we really need this for a QLoRA given its [description](https://huggingface.co/docs/accelerate/v0.19.0/en/usage_guides/fsdp) (I have to read more about it) but it seems it can't hurt, and it's helpful to have the code for future reference. You can always comment out the accelerator if you want to try without."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TEzYBadkyRgd"
      },
      "outputs": [],
      "source": [
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "\n",
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9KNTJZkyRgn"
      },
      "source": [
        "#### Weights & Biases\n",
        "\n",
        "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "DDqUNyIoyRgo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ],
      "source": [
        "#!pip install -q wandb -U\n",
        "\n",
        "import wandb, os\n",
        "wandb.login()\n",
        "os.environ[\"WANDB_NOTEBOOK_NAME \"] = \"finetune\"\n",
        "wandb_project = \"trainbook\"\n",
        "if len(wandb_project) > 0:\n",
        "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "s6f4z8EYmcJ6"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/load.py:2556\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2552\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2553\u001b[0m )\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2556\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2573\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/load.py:2228\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2226\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   2227\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 2228\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   2241\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/load.py:1762\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# We have several ways to get a dataset builder:\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# - if path is the name of a packaged dataset module\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \n\u001b[1;32m   1760\u001b[0m \u001b[38;5;66;03m# Try packaged\u001b[39;00m\n\u001b[1;32m   1761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES:\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPackagedDatasetModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;66;03m# Try locally\u001b[39;00m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(filename):\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/load.py:1129\u001b[0m, in \u001b[0;36mPackagedDatasetModuleFactory.__init__\u001b[0;34m(self, name, data_dir, data_files, download_config, download_mode)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config \u001b[38;5;241m=\u001b[39m download_config\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_mode \u001b[38;5;241m=\u001b[39m download_mode\n\u001b[0;32m-> 1129\u001b[0m \u001b[43mincrease_load_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/load.py:285\u001b[0m, in \u001b[0;36mincrease_load_count\u001b[0;34m(name, resource_type)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mHF_DATASETS_OFFLINE \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39mHF_UPDATE_DOWNLOAD_COUNTS:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m         \u001b[43mhead_hf_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresource_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/utils/file_utils.py:100\u001b[0m, in \u001b[0;36mhead_hf_s3\u001b[0;34m(identifier, filename, use_cdn, dataset, max_retries)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhead_hf_s3\u001b[39m(\n\u001b[1;32m     98\u001b[0m     identifier: \u001b[38;5;28mstr\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, use_cdn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[requests\u001b[38;5;241m.\u001b[39mResponse, \u001b[38;5;167;01mException\u001b[39;00m]:\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_head\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_bucket_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cdn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cdn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/utils/file_utils.py:419\u001b[0m, in \u001b[0;36mhttp_head\u001b[0;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[1;32m    417\u001b[0m headers \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(headers) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    418\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_datasets_user_agent(user_agent\u001b[38;5;241m=\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 419\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/utils/file_utils.py:304\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    302\u001b[0m tries \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectTimeout, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/connection.py:653\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[1;32m    651\u001b[0m server_hostname_rm_dot \u001b[38;5;241m=\u001b[39m server_hostname\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 653\u001b[0m sock_and_verified \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock_and_verified\u001b[38;5;241m.\u001b[39msocket\n\u001b[1;32m    673\u001b[0m \u001b[38;5;66;03m# Forwarding proxies can never have a verified target since\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;66;03m# the proxy is the one doing the verification. Should instead\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# use a CONNECT tunnel in order to verify the target.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# See: https://github.com/urllib3/urllib3/issues/3267.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/connection.py:806\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[1;32m    804\u001b[0m         server_hostname \u001b[38;5;241m=\u001b[39m normalized\n\u001b[0;32m--> 806\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/util/ssl_.py:465\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:  \u001b[38;5;66;03m# Defensive: in CI, we always have set_alpn_protocols\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/urllib3/util/ssl_.py:509\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    506\u001b[0m     SSLTransport\u001b[38;5;241m.\u001b[39m_validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.12/ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.12/ssl.py:1042\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   1040\u001b[0m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1042\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/usr/local/lib/python3.12/ssl.py:1320\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1320\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset('json', data_files='train.jsonl', split='train')\n",
        "eval_dataset = load_dataset('json', data_files='val.jsonl', split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhw8JiOr3m18"
      },
      "source": [
        "#### Formatting prompts\n",
        "Then create a `formatting_func` to structure training examples as prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-fJR0MlQiTD"
      },
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "    text = f\"### Generate Scifi: {example['input']} ### Output:{example['output']}\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sflV0DL2P64_"
      },
      "source": [
        "Here's another common one:\n",
        "\n",
        "```python\n",
        "def formatting_func(example):\n",
        "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
        "    return text\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shz8Xdv-yRgf"
      },
      "source": [
        "### 3. Load Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ-5idQwzvg-"
      },
      "source": [
        "Let's now load Phi-2 using 8-bit quantization!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "45524c98039a46d5b7745ad7cb638d2f"
          ]
        },
        "id": "E0Nl5mWL0k2T",
        "outputId": "47b6b01d-e9f2-4b70-919c-17ae64993843"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9ffd28c82544f60a9f818371239e6fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08f78a47d54f41838f3a00f24882458d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
            "- configuration_phi.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "985509b61e544f12baa0c4b2521bb7dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
            "- modeling_phi.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1280a1d058f34d7892a101ea52796c3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7088878f168443cbb055c00d4d954b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "578c49c27d9e48eaaa3277125a2dcb43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e67979c4c0f495488cfdea45361d6c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aac53130b3174c548f65bf8852ed8050",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81fedd050efa4ae980cd5125b6dc69e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "base_model_id = \"microsoft/phi-2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, load_in_8bit=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjNdXolqyRgf"
      },
      "source": [
        "### 4. Tokenization\n",
        "\n",
        "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
        "\n",
        "\n",
        "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haSUDD9HyRgf",
        "outputId": "22ee95db-2974-4ab0-e0c7-444d04d3e838"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6b0a2b30b44460e961d19e35afff43e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c0bd8a15b5147e4b26755635a9bc3f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "358f013fa40e4891a795a4c16815a4de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "523337fce53c49c093c9030012a3f92b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa8d9c3f73ea4eb58b78406a786fc7ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6e7764006bc4d39a85e05317569bf3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    use_fast=False, # needed for now, should be fixed soon\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_and_tokenize_prompt(prompt):\n",
        "    return tokenizer(formatting_func(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHnKLcq4yRgg"
      },
      "source": [
        "Reformat the prompt and tokenize each sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3iLAwLh3m19"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d9cc53614c949158ead810bf9e2bcf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b00d9893ebb4125843e2db807f1aa6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6ewk27p3m19"
      },
      "source": [
        "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA8M9yfC3m19",
        "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
        "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBk4Qp_vyRgh"
      },
      "source": [
        "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs.\n",
        "\n",
        "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMlw8h743m19"
      },
      "source": [
        "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acINaViR3m19"
      },
      "outputs": [],
      "source": [
        "max_length = 260 # This was an appropriate max length for my dataset\n",
        "\n",
        "def generate_and_tokenize_prompt2(prompt):\n",
        "    result = tokenizer(\n",
        "        formatting_func(prompt),\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "518d4f0b89bf4d57bf00d4c6d6e59eb5"
          ]
        },
        "id": "lTk-aTog3m19",
        "outputId": "4fb637b4-77a2-47c6-de7b-4fb620663dd7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5532ec15c30d4872839ce30f63e60c36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9483618597474d4d88ad582302a06dc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQL796OayRgh"
      },
      "source": [
        "Generally, each `input_ids` should be padded on the left with the `eos_token` (50256) and there should be an `eos_token` 50256 added to the end, and the prompt should start with a `bos_token` (?). However, I'm getting an error with Phi-2's tokenizer. GPU credits for whoever can resolve this!\n",
        "\n",
        "Hopefully should work just fine as-is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKHhvxK83m19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 21017, 2980, 378, 1446, 22238, 25, 44386, 1446, 22238, 25, 15684, 25, 5524, 4257, 3706, 520, 7899, 11, 16299, 991, 287, 10326, 6450, 13, 2295, 9650, 453, 1288, 515, 706, 4388, 10325, 379, 6439, 10965, 13, 39373, 4892, 4634, 355, 39497, 46045, 351, 6190, 3037, 30468, 13, 7561, 9018, 1278, 78, 803, 625, 29340, 13, 10286, 12, 10547, 4847, 2291, 36043, 3037, 290, 43286, 10325, 13, 45362, 25, 28921, 2870, 351, 281, 739, 14421, 286, 13479, 13, 569, 1699, 6764, 25, 8407, 4252, 5621, 625, 50244, 6450, 6794, 11, 520, 7899, 338, 1767, 269, 4335, 287, 34954, 355, 339, 9890, 82, 287, 262, 44005, 9550, 286, 465, 649, 21334, 13, 44386, 25235, 25, 198, 1273, 7899, 3830, 991, 287, 262, 6450, 810, 339, 3214, 11, 1278, 78, 803, 625, 262, 1943, 198, 1659, 465, 10325, 13]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_train_dataset[1]['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6LRa2Zm3m19"
      },
      "source": [
        "Now all the samples should be the same length, `max_length`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I55Yo3yy3m19",
        "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP3R4enP3m19"
      },
      "source": [
        "### How does the base model do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxbl4ACsyRgi"
      },
      "source": [
        "Optionally, you can check how Phi-2 does on one of your data samples. For example, if you have a dataset of users' biometric data to their health scores, you could test the following `eval_prompt`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOxnx-cAyRgi"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\" Given the following biometric data, score the users' health, from 0-100.\n",
        "\n",
        "### Biometric Data:\n",
        "Temperature=98.2,\n",
        "Sex=F,\n",
        "Age=29,\n",
        "Height=69 inches,\n",
        "Weight=160 lbs,\n",
        "V02_Max=55,\n",
        "HRV=55\n",
        "\n",
        "### Health Score:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRhfq_Fa3m19"
      },
      "source": [
        "The `eval_prompt` I used was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa6ux9ni3m19"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"The following is generated scifi content: a man is sitting on a planet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NidIuFXMyRgi",
        "outputId": "b1794b11-9a22-4b0a-e871-7df039ab59fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is generated scifi content: a man is sitting on a planet, looking at the stars. He's thinking about how he got there and what it means to be alive in this vast universe.\n",
            "This is not science fiction; it's real life. We are all living on planets that were once just like ours - small, rocky worlds orbiting distant stars. And we're still discovering new ones every day!\n",
            "But why do we care so much about these other places? What makes them different from Earth?\n",
            "Well, for one thing, they might have conditions suitable for life as we know it. That would mean finding evidence of water or organic molecules (like amino acids) which could indicate whether some kind of biological process has occurred here before now too!\n",
            "And even if nothing else seems promising yet â€“ maybe because no signs exist whatsoever â€“ scientists will continue searching until something pops up unexpectedly during their research mission.\n",
            "So far, only two planets outside our solar system have been confirmed by direct observation using telescopes such as Hubble Space Telescope (HST). These discoveries came after years of careful observations made with ground-based instruments like Keck Observatory in Hawaii USA & La Silla Observatory Chile where astronomers detected tiny wobbles caused by gravitational pull exerted by unseen objects around them called exoplanets.\n",
            "In addition, indirect detection methods include measuring changes\n"
          ]
        }
      ],
      "source": [
        "# Init an eval tokenizer so it doesn't add padding or eos token\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        "    use_fast=False, # needed for now, should be fixed soon\n",
        ")\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCAWeCzZyRgi"
      },
      "source": [
        "Observe how the model does out of the box. This is clearly not my journal, lol."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AapDoyfAyRgi"
      },
      "source": [
        "### 5. Set Up LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp2gMi1ZzGET"
      },
      "source": [
        "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. Let's set up our LoRA layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkIcwsSU01EB"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUYEpEK-yRgj"
      },
      "source": [
        "Let's print the model to examine its layers, as we will apply QLoRA to some linear layers of the model. Those layers are `Wqkv`, `fc1`, `fc2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XshGNsbxyRgj",
        "outputId": "c619b0e8-8516-4d4b-9abe-13eaa3f3b204",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PhiForCausalLM(\n",
            "  (model): PhiModel(\n",
            "    (embed_tokens): Embedding(51200, 2560)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x PhiDecoderLayer(\n",
            "        (self_attn): PhiAttention(\n",
            "          (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "          (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "          (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "          (rotary_emb): PhiRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): PhiMLP(\n",
            "          (activation_fn): NewGELUActivation()\n",
            "          (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
            "          (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
            "        )\n",
            "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6mTLuQJyRgj"
      },
      "source": [
        "Here we define the LoRA config.\n",
        "\n",
        "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
        "\n",
        "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
        "\n",
        "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybeyl20n3dYH",
        "outputId": "6a16c182-04d9-4812-ae81-502a8fe364d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 26214400 || all params: 2805898240 || trainable%: 0.9342605382581515\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"Wqkv\",\n",
        "        \"fc1\",\n",
        "        \"fc2\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_FHi_VLyRgn"
      },
      "source": [
        "See how the model looks different now, with the LoRA adapters added:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaYMWak4yRgn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): PhiForCausalLM(\n",
            "      (model): PhiModel(\n",
            "        (embed_tokens): Embedding(51200, 2560)\n",
            "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x PhiDecoderLayer(\n",
            "            (self_attn): PhiAttention(\n",
            "              (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "              (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "              (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "              (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n",
            "              (rotary_emb): PhiRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): PhiMLP(\n",
            "              (activation_fn): NewGELUActivation()\n",
            "              (fc1): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=10240, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (fc2): lora.Linear8bitLt(\n",
            "                (base_layer): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=10240, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "            )\n",
            "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0MOtwf3zdZp"
      },
      "source": [
        "### 6. Run Training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEe0uWYSyRgo"
      },
      "source": [
        "I didn't have a lot of training samples: only about 200 total train/validation. I used 500 training steps, and I was fine with overfitting in this case. I found that the end product worked well. It took about 20 minutes on the 1x A10G 24GB.\n",
        "\n",
        "Overfitting is when the validation loss goes up (bad) while the training loss goes down significantly, meaning the model is learning the training set really well, but is unable to generalize to new datapoints. In most cases, this is not desired, but since I am just playing around with a model to generate outputs like my journal entries, I was fine with a moderate amount of overfitting.\n",
        "\n",
        "With that said, a note on training: you can set the `max_steps` to be high initially, and examine at what step your model's performance starts to degrade. There is where you'll find a sweet spot for how many steps to perform. For example, say you start with 1000 steps, and find that at around 500 steps the model starts overfitting, as described above. Therefore, 500 steps would be your sweet spot, so you would use the `checkpoint-500` model repo in your output dir (`phi2-journal-finetune`) as your final model in step 6 below.\n",
        "\n",
        "If you're just doing something for fun like I did and are OK with overfitting, you can try different checkpoint versions with different degrees of overfitting.\n",
        "\n",
        "You can interrupt the process via Kernel -> Interrupt Kernel in the top nav bar once you realize you didn't need to train anymore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxSbpKQSLY6B"
      },
      "outputs": [],
      "source": [
        "model = accelerator.prepare_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_L1131GyRgo"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq0nX33BmfaC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspaces/tuned/wip/wandb/run-20240327_054612-gr1i968v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='http://localhost:8080/jak/trainbook/runs/gr1i968v/workspace' target=\"_blank\">phi2-journal-finetune-2024-03-27-05-46</a></strong> to <a href='http://localhost:8080/jak/trainbook' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='http://localhost:8080/jak/trainbook' target=\"_blank\">http://localhost:8080/jak/trainbook</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='http://localhost:8080/jak/trainbook/runs/gr1i968v/workspace' target=\"_blank\">http://localhost:8080/jak/trainbook/runs/gr1i968v/workspace</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='196' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [196/500 01:10 < 01:50, 2.76 it/s, Epoch 48.75/125]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.128100</td>\n",
              "      <td>2.807312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.453800</td>\n",
              "      <td>2.463475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.819000</td>\n",
              "      <td>2.441019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.291000</td>\n",
              "      <td>2.626316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.853500</td>\n",
              "      <td>2.865957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.504200</td>\n",
              "      <td>3.106334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.276200</td>\n",
              "      <td>3.410799</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 35\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:1806\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2150\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2149\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2150\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2153\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2154\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2155\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2156\u001b[0m ):\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:3077\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3075\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3076\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3077\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from datetime import datetime\n",
        "\n",
        "project = \"journal-finetune\"\n",
        "base_model_name = \"phi2\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        warmup_steps=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        max_steps=500,\n",
        "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,              # When to start reporting loss\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=25,                # Save checkpoints every 50 steps\n",
        "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training\n",
        "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D57XqcsyRgo"
      },
      "source": [
        "### 7. Drum Roll... Try the Trained Model!\n",
        "\n",
        "It's a good idea to kill the current process so that you don't run out of memory loading the base model again on top of the model we just trained. Go to `Kernel > Restart Kernel` or kill the process via the Terminal (`nvidia smi` > `kill [PID]`). \n",
        "\n",
        "By default, the PEFT library will only save the QLoRA adapters, so we need to first load the base model from the Huggingface Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "fb8230fb86884aa6be318e2d03a88af2"
          ]
        },
        "id": "SKSnF016yRgp",
        "outputId": "bce5209d-90da-4117-c6ac-cda9f3cb3422"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "287b9a6585694eb9b8126001c96c61e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "base_model_id = \"microsoft/phi-2\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Phi2, same as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
        "eval_tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BxOhAiqyRgp"
      },
      "source": [
        "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GwsiqhWuyRgp"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"phi2-journal-finetune/checkpoint-100\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX39ibolyRgp"
      },
      "source": [
        "and run your inference!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUehsaVNyRgp"
      },
      "source": [
        "Let's try the same `eval_prompt` and thus `model_input` as above, and see if the new finetuned model performs better. I like playing with the repetition penalty (just little tweaks of .01-.05 at a time). THIS IS SO FUN. I'm obsessed wth this AI version of myself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lMkVNEUvyRgp",
        "outputId": "7d49d409-5dbe-4306-c1a4-9d87e3073397"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The following is generated scifi content: a man is sitting on a planet \n",
            "describing it as barren and desolate, with no signs of life. He reflects on his current situation, having been stranded here for months after an accident during a spacewalk. Despite the lack of resources, he tries to remain optimistic, hoping for rescue or a way back home. As he looks out into the vast emptiness, he wonders if this is just another stop in an endless universe. Suddenly, a small dust cloud moves across the sky, catching his attention. Could it be a sign\n"
          ]
        }
      ],
      "source": [
        "eval_prompt = \" The following is generated scifi content: a man is sitting on a planet \"\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "ft_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.11)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCJnpZoayRgq"
      },
      "source": [
        "### Sweet... it worked! The fine-tuned model now prints out journal entries in my style!\n",
        "\n",
        "How funny to see it write like me as an angsty teenager, and honestly adult. I am obsessed. It knows who my friends are and talks about them, and covers the same topics I usually cover. It's really cool.\n",
        "\n",
        "I hope you enjoyed this tutorial on fine-tuning Microsoft's Phi-2 on your own data. If you have any questions, feel free to reach out to me on [X](https://x.com/harperscarroll) or [Discord](https://discord.gg/RN2a436M73).\n",
        "\n",
        "ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™ ðŸ¤™"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
